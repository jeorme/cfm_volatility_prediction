{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 14 01:30:12 2018\n",
    "\n",
    "@author: xshitova\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn import datasets, linear_model, metrics \n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import Imputer\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "import functools as ft\n",
    "import datetime\n",
    "\n",
    "# function returnes accumulated volatility during day\n",
    "# as SUM(V*P), where V,P vectors of volatility,price for morning day ticks \n",
    "def DayVolatility(ser):\n",
    "    l=ser.size\n",
    "    if l % 2 == 1:\n",
    "        print('DayVolatility: error: serie length is not even:',l)\n",
    "        return np.NaN\n",
    "    l = int(l/2)\n",
    "    v=ser[:l].values # vector of volatilities\n",
    "    p=ser[l:].values # vector of price movements\n",
    "    return ft.reduce(lambda x, y: x + y, v*p)   # sum(V*P)\n",
    "\n",
    "# function returnes maximal volatility jump at a sequential price increase (joint ticks with prices +1,0)\n",
    "def DayPriceUpMax(ser):\n",
    "    l=ser.size\n",
    "    if l % 2 == 1:\n",
    "        print('DayVolatility: error: serie length is not even:',l)\n",
    "        return np.NaN\n",
    "    l = int(l/2)\n",
    "    vol=ser[:l].values # vector of volatilities\n",
    "    pm=ser[l:].values # vector of price movements\n",
    "    vmax=0; vcur=0;\n",
    "    for i,v in enumerate(vol):\n",
    "        if pd.isnull(v):\n",
    "            continue\n",
    "        if pm[i]>0:                 # check up price movement\n",
    "            vcur += v               # accumulate volatility\n",
    "            if vcur > vmax:\n",
    "                vmax = vcur\n",
    "        elif pm[i]<0:               # reset volatility if price falls\n",
    "            vcur=0\n",
    "    return vmax\n",
    "\n",
    "# function returnes maximal volatility jump at a sequential price decrease (joint ticks with prices -1,0)\n",
    "def DayPriceDownMax(ser):\n",
    "    l=ser.size\n",
    "    if l % 2 == 1:\n",
    "        print('DayVolatility: error: serie length is not even:',l)\n",
    "        return np.NaN\n",
    "    l = int(l/2)\n",
    "    vol=ser[:l].values # vector of volatilities\n",
    "    pm=ser[l:].values # vector of price movements\n",
    "    vmax=0; vcur=0;\n",
    "    for i,v in enumerate(vol):\n",
    "        if pd.isnull(v):\n",
    "            continue\n",
    "        if pm[i]<0:                 # check down price movement\n",
    "            vcur += v               # accumulate volatility\n",
    "            if vcur > vmax:\n",
    "                vmax = vcur\n",
    "        elif pm[i]>0:               # reset volatility if price ups\n",
    "            vcur=0\n",
    "    return vmax\n",
    "\n",
    "# function returnes total volatility in price up ticks\n",
    "def DayPriceUpTot(ser):\n",
    "    l=ser.size\n",
    "    if l % 2 == 1:\n",
    "        print('DayVolatility: error: serie length is not even:',l)\n",
    "        return np.NaN\n",
    "    l = int(l/2)\n",
    "    vol=ser[:l].values # vector of volatilities\n",
    "    pm=ser[l:].values # vector of price movements\n",
    "    vmax=0\n",
    "    for i,v in enumerate(vol):\n",
    "        if pd.isnull(v):\n",
    "            continue\n",
    "        if pm[i]>0:                 # check up price movement\n",
    "            vmax += v               # accumulate volatility\n",
    "    return vmax\n",
    "\n",
    "# function returnes total volatility in price down ticks\n",
    "def DayPriceDownTot(ser):\n",
    "    l=ser.size\n",
    "    if l % 2 == 1:\n",
    "        print('DayVolatility: error: serie length is not even:',l)\n",
    "        return np.NaN\n",
    "    l = int(l/2)\n",
    "    vol=ser[:l].values # vector of volatilities\n",
    "    pm=ser[l:].values # vector of price movements\n",
    "    vmax=0\n",
    "    for i,v in enumerate(vol):\n",
    "        if pd.isnull(v):\n",
    "            continue\n",
    "        if pm[i]<0:                 # check down price movement\n",
    "            vmax += v               # accumulate volatility\n",
    "    return vmax\n",
    "\n",
    "# function returnes average volatility in price up ticks\n",
    "def DayPriceUpTotAvg(ser):\n",
    "    l=ser.size\n",
    "    if l % 2 == 1:\n",
    "        print('DayVolatility: error: serie length is not even:',l)\n",
    "        return np.NaN\n",
    "    l = int(l/2)\n",
    "    vol=ser[:l].values # vector of volatilities\n",
    "    pm=ser[l:].values # vector of price movements\n",
    "    vmax=0; n=0\n",
    "    for i,v in enumerate(vol):\n",
    "        if pd.isnull(v):\n",
    "            continue\n",
    "        if pm[i]>0:                 # check up price movement\n",
    "            vmax += v               # accumulate volatility\n",
    "            n += 1\n",
    "    if n<=0:\n",
    "        n=1\n",
    "    return vmax/n\n",
    "\n",
    "# function returnes average volatility in price down ticks\n",
    "def DayPriceDownTotAvg(ser):\n",
    "    l=ser.size\n",
    "    if l % 2 == 1:\n",
    "        print('DayVolatility: error: serie length is not even:',l)\n",
    "        return np.NaN\n",
    "    l = int(l/2)\n",
    "    vol=ser[:l].values # vector of volatilities\n",
    "    pm=ser[l:].values # vector of price movements\n",
    "    vmax=0; n=0\n",
    "    for i,v in enumerate(vol):\n",
    "        if pd.isnull(v):\n",
    "            continue\n",
    "        if pm[i]<0:                 # check down price movement\n",
    "            vmax += v               # accumulate volatility\n",
    "            n += 1\n",
    "    if n<=0:\n",
    "        n=1\n",
    "    return vmax/n\n",
    "\n",
    "#uploading the files with data \n",
    "input_train=pd.read_csv('C:/Users/xshitova/Downloads/training_input.csv', delimiter=';')\n",
    "output_train=pd.read_csv('C:/Users/xshitova/Downloads/challenge_output_data_training_file_volatility_prediction_in_financial_markets.csv', delimiter=';') \n",
    "\n",
    "input_test=pd.read_csv('C:/Users/xshitova/Downloads/testing_input.csv', delimiter=';')\n",
    "\n",
    "#note to self - the train and the test are actually of the same size \n",
    "\n",
    "#column names in inputs \n",
    "#ID is the name of the line and primary key here\n",
    "cols=[\"ID\",\"date\",\"product_id\",\"volatility 09:30:00\",\"volatility 09:35:00\",\"volatility 09:40:00\",\n",
    "      \"volatility 09:45:00\",\"volatility 09:50:00\",\"volatility 09:55:00\",\"volatility 10:00:00\",\"volatility 10:05:00\",\n",
    "      \"volatility 10:10:00\",\"volatility 10:15:00\",\"volatility 10:20:00\",\"volatility 10:25:00\",\"volatility 10:30:00\",\n",
    "      \"volatility 10:35:00\",\"volatility 10:40:00\",\"volatility 10:45:00\",\"volatility 10:50:00\",\"volatility 10:55:00\",\n",
    "      \"volatility 11:00:00\",\"volatility 11:05:00\",\"volatility 11:10:00\",\"volatility 11:15:00\",\"volatility 11:20:00\",\n",
    "      \"volatility 11:25:00\",\"volatility 11:30:00\",\"volatility 11:35:00\",\"volatility 11:40:00\",\"volatility 11:45:00\",\n",
    "      \"volatility 11:50:00\",\"volatility 11:55:00\",\"volatility 12:00:00\",\"volatility 12:05:00\",\"volatility 12:10:00\",\n",
    "      \"volatility 12:15:00\",\"volatility 12:20:00\",\"volatility 12:25:00\",\"volatility 12:30:00\",\"volatility 12:35:00\",\n",
    "      \"volatility 12:40:00\",\"volatility 12:45:00\",\"volatility 12:50:00\",\"volatility 12:55:00\",\"volatility 13:00:00\",\n",
    "      \"volatility 13:05:00\",\"volatility 13:10:00\",\"volatility 13:15:00\",\"volatility 13:20:00\",\"volatility 13:25:00\",\n",
    "      \"volatility 13:30:00\",\"volatility 13:35:00\",\"volatility 13:40:00\",\"volatility 13:45:00\",\"volatility 13:50:00\",\n",
    "      \"volatility 13:55:00\",\n",
    "      \"return 09:30:00\",\"return 09:35:00\",\"return 09:40:00\",\"return 09:45:00\",\"return 09:50:00\",\"return 09:55:00\",\n",
    "      \"return 10:00:00\",\"return 10:05:00\",\"return 10:10:00\",\"return 10:15:00\",\"return 10:20:00\",\"return 10:25:00\",\n",
    "      \"return 10:30:00\",\"return 10:35:00\",\"return 10:40:00\",\"return 10:45:00\",\"return 10:50:00\",\"return 10:55:00\",\n",
    "      \"return 11:00:00\",\"return 11:05:00\",\"return 11:10:00\",\"return 11:15:00\",\"return 11:20:00\",\"return 11:25:00\",\n",
    "      \"return 11:30:00\",\"return 11:35:00\",\"return 11:40:00\",\"return 11:45:00\",\"return 11:50:00\",\"return 11:55:00\",\n",
    "      \"return 12:00:00\",\"return 12:05:00\",\"return 12:10:00\",\"return 12:15:00\",\"return 12:20:00\",\"return 12:25:00\",\n",
    "      \"return 12:30:00\",\"return 12:35:00\",\"return 12:40:00\",\"return 12:45:00\",\"return 12:50:00\",\"return 12:55:00\",\n",
    "      \"return 13:00:00\",\"return 13:05:00\",\"return 13:10:00\",\"return 13:15:00\",\"return 13:20:00\",\"return 13:25:00\",\n",
    "      \"return 13:30:00\",\"return 13:35:00\",\"return 13:40:00\",\"return 13:45:00\",\"return 13:50:00\",\"return 13:55:00\"]\n",
    "\n",
    "#column names in outputs \n",
    "cols1=[\"ID\",\"TARGET\"]\n",
    "\n",
    "#taking all the volatilities for each line and calculating their mean\n",
    "col = input_train.loc[: , \"volatility 09:30:00\":\"volatility 13:55:00\"]\n",
    "output_train['PREDICTED'] = col.mean(axis=1)\n",
    "input_train['MEAN']=col.mean(axis=1)\n",
    "\n",
    "#simple OLS model with a non-binary output, drop missing \n",
    "model = sm.OLS(output_train['TARGET'],input_train[cols], missing='drop')\n",
    "results=model.fit()\n",
    "output_train['TARGET1']  = results.predict(input_train[cols]) \n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print('Starting to calculate variables')\n",
    "print(datetime.datetime.now()-start_time)\n",
    "\n",
    "# create new dataframe with calculated features from original data\n",
    "nf=pd.DataFrame()\n",
    "nf['TARGET']=output_train['TARGET'].values #add target volativility into the dataframe \n",
    "nf['VOL_MEAN']=input_train.loc[:,\"volatility 09:30:00\":\"volatility 13:55:00\"].mean(axis=1).values #add mean volatility into the dataframe\n",
    "nf['VOL_MAX']=input_train.loc[:,\"volatility 09:30:00\":\"volatility 13:55:00\"].max(axis=1).values #add max volatility into the dataframe\n",
    "\n",
    "# take price movement data into separate dataset for processing\n",
    "pmv=input_train.loc[:,\"return 09:30:00\":\"return 13:55:00\"].copy()\n",
    "pmv.fillna(0) #fill all empty values by zeros \n",
    "pmv_up=pmv.replace(1.0,np.NaN) # setup up going prices as missed values in order to count them later on \n",
    "nf['Sall-Up']=pmv_up.count(axis=1).values # intermediate variable = total non-missed - number of bars with price going up\n",
    "pmv_down=pmv.replace(-1.0,np.NaN) # setup up going prices as missed values in order to count them further\n",
    "nf['Sall-Down']=pmv_down.count(axis=1).values # intermediate variable = total non-missed - number of bars with price going down\n",
    "nf['rp_up']=(54. - nf['Sall-Up'])/54. #true number of ticks where price goes up \n",
    "nf['rp_down']=(54. - nf['Sall-Down'])/54. #true number of ticks where price goes down \n",
    "nf.drop(columns=['Sall-Up','Sall-Down'],inplace=True) #deleting intermediate variables \n",
    "\n",
    "# accumulated volatility per day\n",
    "nf['v_acc']=input_train.loc[:,\"volatility 09:30:00\":\"return 13:55:00\"].agg(DayVolatility,axis=1).values\n",
    "# maximal volatility up movement in a single price up spike during the day\n",
    "nf['v_up_max']=input_train.loc[:,\"volatility 09:30:00\":\"return 13:55:00\"].agg(DayPriceUpMax,axis=1).values\n",
    "# maximal volatility down movement in a single price down spike during the day\n",
    "nf['v_down_max']=input_train.loc[:,\"volatility 09:30:00\":\"return 13:55:00\"].agg(DayPriceDownMax,axis=1).values\n",
    "# total volatility accumulated in price up ticks\n",
    "nf['v_up_tot']=input_train.loc[:,\"volatility 09:30:00\":\"return 13:55:00\"].agg(DayPriceUpTot,axis=1).values\n",
    "# total volatility accumulated in price down ticks\n",
    "nf['v_down_tot']=input_train.loc[:,\"volatility 09:30:00\":\"return 13:55:00\"].agg(DayPriceDownTot,axis=1).values\n",
    "# average volatility accumulated in price up ticks\n",
    "nf['v_up_tot_avg']=input_train.loc[:,\"volatility 09:30:00\":\"return 13:55:00\"].agg(DayPriceUpTotAvg,axis=1).values\n",
    "# average volatility accumulated in price down ticks\n",
    "nf['v_down_tot_avg']=input_train.loc[:,\"volatility 09:30:00\":\"return 13:55:00\"].agg(DayPriceDownTotAvg,axis=1).values\n",
    "\n",
    "# asymmetry parameters\n",
    "nf['rp_asym']=(nf['rp_up'] - nf['rp_down'])/(nf['rp_up'] + nf['rp_down'])\n",
    "nf['v_max_asym']=(nf['v_up_max'] - nf['v_down_max'])/(nf['v_up_max'] + nf['v_down_max'])\n",
    "nf['v_tot_asym']=(nf['v_up_tot'] - nf['v_down_tot'])/(nf['v_up_tot'] + nf['v_down_tot'])\n",
    "nf['v_tot_avg_asym']=(nf['v_up_tot_avg'] - nf['v_down_tot_avg'])/(nf['v_up_tot_avg'] + nf['v_down_tot_avg'])\n",
    "\n",
    "cols_new=['VOL_MEAN','VOL_MAX','rp_up','rp_down','v_acc','v_up_max','v_down_max',\n",
    "          'v_up_tot','v_down_tot','v_up_tot_avg','v_down_tot_avg','rp_asym','v_max_asym','v_tot_asym','v_tot_avg_asym'] #all columns for prediction\n",
    "\n",
    "print('Variables calculated, prediction starting....')\n",
    "print(datetime.datetime.now()-start_time)\n",
    "\n",
    "nf[cols_new].fillna(0) #fill all empty values by zeros\n",
    "#linear regression model - aggregated variables\n",
    "model_linreg = LinearRegression()\n",
    "model_linreg.fit(nf[cols_new], nf['TARGET'])\n",
    "nf['TARGET_4VAR'] = model_linreg.predict(nf[cols_new]) \n",
    "\n",
    "#simple OLS model with a non-binary output, - aggregated variables\n",
    "model1 = sm.OLS(nf['TARGET'],nf[cols_new])\n",
    "results1=model.fit()\n",
    "nf['TARGET_OLS_4VAR']  = results1.predict(nf[cols_new]) \n",
    "\n",
    "print(\"\")\n",
    "#calculate model errors on train and print it out for each model separately\n",
    "model_error= np.mean(np.abs((output_train['TARGET'] - output_train['PREDICTED']) / output_train['TARGET'])) * 100\n",
    "print(\"Error is \"+str(model_error)+\" for mean\")\n",
    "model_error_ols= np.mean(np.abs((output_train['TARGET'] - output_train['TARGET1']) / output_train['TARGET'])) * 100\n",
    "print(\"Error is \"+str(model_error_ols)+\" for OLS, missing values ommitted\")\n",
    "model_error_lr4=np.mean(np.abs((nf['TARGET'] - nf['TARGET_4VAR']) / nf['TARGET'])) * 100\n",
    "print(\"Error is \"+str(model_error_lr4)+\" for LR, fifteen aggregated variables\")\n",
    "model_error_ols4=np.mean(np.abs((nf['TARGET'] - nf['TARGET_OLS_4VAR']) / nf['TARGET'])) * 100\n",
    "print(\"Error is \"+str(model_error_ols4)+\" for OLS, fifteen aggregated variables\")\n",
    "\n",
    "\n",
    "print(datetime.datetime.now()-start_time) #final time \n",
    "\n",
    "\n",
    "\n",
    "#autoregression model\n",
    "#input_train.index = pd.to_datetime(pd.Index(input_train.index))\n",
    "#model = AR(input_train, missing='drop') \n",
    "#results_ar = model.fit()\n",
    "#output_train['TARGET_AR']  = results_ar.predict(dymanic=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
